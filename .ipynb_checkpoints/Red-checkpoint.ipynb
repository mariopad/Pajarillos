{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf68c86b-9cde-41cf-aa4b-17d0281ec676",
   "metadata": {},
   "source": [
    "# üìå Explicaci√≥n de Gradient-Based Training en Redes Neuronales\n",
    "\n",
    "## **1Ô∏è‚É£ Definiendo la funci√≥n de p√©rdida**\n",
    "En una red neuronal, tenemos un conjunto de par√°metros **Œ∏ (pesos y sesgos)** y queremos minimizar una **funci√≥n de p√©rdida** $L(\\theta)$. Para una tarea de clasificaci√≥n multiclase, usamos **entrop√≠a cruzada**:\n",
    "\n",
    "$$\n",
    "L(\\theta) = -\\sum_{i=1}^{N} y_i \\log(\\hat{y}_i)\n",
    "$$\n",
    "\n",
    "donde:\n",
    "- $ y_i $ es la etiqueta real (codificada en one-hot si hay varias clases).\n",
    "- $ \\hat{y}_i $ es la probabilidad predicha por la red (salida de la funci√≥n softmax).\n",
    "\n",
    "El objetivo es encontrar los pesos $ \\theta $ que minimicen $ L(\\theta) $.\n",
    "\n",
    "---\n",
    "\n",
    "## **2Ô∏è‚É£ C√°lculo del Gradiente y Descenso del Gradiente**\n",
    "Para minimizar $ L(\\theta) $, calculamos su **gradiente**:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta L(\\theta) = \\left( \\frac{\\partial L}{\\partial \\theta_1}, \\frac{\\partial L}{\\partial \\theta_2}, \\dots, \\frac{\\partial L}{\\partial \\theta_n} \\right)\n",
    "$$\n",
    "\n",
    "El gradiente indica **la direcci√≥n de m√°ximo aumento de la p√©rdida**. Queremos movernos en la direcci√≥n opuesta:\n",
    "\n",
    "$$\n",
    "\\theta \\leftarrow \\theta - \\eta \\nabla_\\theta L(\\theta)\n",
    "$$\n",
    "\n",
    "donde **$ \\eta $ (tasa de aprendizaje)** es un hiperpar√°metro que controla cu√°nto modificamos los pesos en cada paso.\n",
    "\n",
    "---\n",
    "\n",
    "## **3Ô∏è‚É£ Backpropagation y actualizaci√≥n de pesos**\n",
    "Durante el entrenamiento, usamos **backpropagation** para propagar los errores hacia atr√°s a trav√©s de la red:\n",
    "\n",
    "1. **Capa de salida:**\n",
    "   - Si usamos **softmax**, la derivada de la p√©rdida respecto a su entrada $ z_i $ es:\n",
    "\n",
    "     $$\n",
    "     \\frac{\\partial L}{\\partial z_i} = \\hat{y}_i - y_i\n",
    "     $$\n",
    "\n",
    "   - Esto indica cu√°nto ajustar cada peso para mejorar la clasificaci√≥n.\n",
    "\n",
    "2. **Capas ocultas densas:**\n",
    "   - Aplicamos la **regla de la cadena** para propagar gradientes hacia atr√°s.\n",
    "   - Si usamos **ReLU** ($ f(z) = \\max(0, z) $), su derivada es:\n",
    "\n",
    "     $$\n",
    "     f'(z) =\n",
    "     \\begin{cases} \n",
    "     1, & \\text{si } z > 0 \\\\\n",
    "     0, & \\text{si } z \\leq 0\n",
    "     \\end{cases}\n",
    "     $$\n",
    "\n",
    "   - Esto significa que **las neuronas inactivas no contribuyen al ajuste de pesos**.\n",
    "\n",
    "---\n",
    "\n",
    "## **4Ô∏è‚É£ Relaci√≥n con EfficientNetB3**\n",
    "En **transfer learning**, EfficientNet ya tiene caracter√≠sticas aprendidas en **ImageNet**, por lo que:\n",
    "\n",
    "- Primero **congelamos** las capas base (sus pesos no se actualizan).\n",
    "- Entrenamos solo las nuevas **capas densas** agregadas.\n",
    "- Luego, en **fine-tuning**, **descongelamos gradualmente las capas previas** y seguimos optimizando con una tasa de aprendizaje peque√±a.\n",
    "\n",
    "---\n",
    "\n",
    "## **üìå Resumen**\n",
    "‚úÖ **Entrenar una red neuronal** implica minimizar una funci√≥n de p√©rdida usando **descenso del gradiente**.  \n",
    "‚úÖ **Backpropagation** propaga los gradientes hacia atr√°s para actualizar los pesos.  \n",
    "‚úÖ En **EfficientNetB3**, primero entrenamos capas nuevas y luego afinamos toda la red.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe929f13-6b0d-498a-adba-c4211f88d172",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras\n",
    "#!pip install tensorflow_datasets\n",
    "\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c514b4c0-2a98-4611-815d-b46d3021f70c",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b619a-b9dd-4cd3-a778-1ce107e634b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Base model\n",
    "base_model = keras.applications.EfficientNetB3(\n",
    "    weights='imagenet',  # Load weights pre-trained on ImageNet.\n",
    "    input_shape=(300, 300, 3), # 300x300 RGB\n",
    "    include_top=False)  # Do not include the ImageNet classifier at the top.\n",
    "\n",
    "base_model.trainable = False # base_model running in inference mode\n",
    "\n",
    "# New model\n",
    "x = GlobalAveragePooling2D()(base_model.output) # Reduce dimensionalidad del resultado\n",
    "# layers with 256 and 128 neurons fully connected (Dense()). ReLU para permitir el aprendizaje de relaciones no lineales\n",
    "# Se suele elegir un decremento en potencias de 2 (por optimizar la utilizacion de GPUs)\n",
    "x = Dense(128, activation=\"relu\")(x)\n",
    "x = Dense(256, activation=\"relu\")(x)\n",
    "# √öltima capa que nos da un array de probabilidades de cada tipo de ave (num_classes)\n",
    "output = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Crear modelo final\n",
    "model = Model(inputs=base_model.input, outputs=output)\n",
    "\n",
    "# Ver estructura\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70dcf4-3e71-4e43-9727-cae689d5619a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compilar\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
